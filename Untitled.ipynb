{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student name: Joseph Hinga Mwangi\n",
    "\n",
    "    \n",
    "# Student pace: Full time \n",
    "\n",
    "    \n",
    "# Scheduled project review date/time:0800hrs\n",
    "\n",
    "\n",
    "# Instructor name: Asha Deen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # EXPLORING CRASH DATA AND PREDICTING SEVERITY\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicle mishaps are a major source of stress for open security and transportation offices. They not as it were cause passing and property harm, but moreover hinder activity stream and cause financial misfortunes. Understanding the components that contribute to these collisions and their impacts is basic for putting viable measures in put to diminish their event and affect.\n",
    "\n",
    "The City of Chicago has collected broad crash information through its electronic crash announcing framework (E-Crash), giving a important asset to dissect and pick up bits of knowledge into the components contributing to mishaps. The dataset comprises a wide run of crash parameters, counting crash circumstances, vehicles included, and individuals influenced.\n",
    "\n",
    "The objective of this consider is to dissect the dataset and give a full understanding of car crashes and their characteristics within the city of Chicago. We trust to find key variables that contribute to collisions, degree the affect they have, and examine the circumstances encompassing the mishaps by considering the various characteristics related with each crash occasion.\n",
    "\n",
    "This project's target group of onlookers incorporates various street security partners such as transportation specialists, law requirement offices, policymakers, and protections companies. Decision-makers can set up centered plans and exercises to diminish the recurrence and seriousness of mischances by knowing the elemental components that contribute to collisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS PROBLEM\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicle crashes within the city of Chicago are a major source of stretch and budgetary burden for different partners, counting transportation specialists, law authorization, policymakers, and protections companies. These crashes not as it were result in misfortune of life and property harm, but moreover disturb activity stream and cause noteworthy financial misfortunes. To viably relieve the recurrence and seriousness of these crashes, decision-makers require a comprehensive understanding of the key components and characteristics contributing to these occurrences.\n",
    "\n",
    "The need of nitty gritty experiences into the root causes and impacts of vehicle crashes in Chicago may be a critical challenge for these partners. Without this information, they are incapable to create and execute focused on procedures and intercessions to address the issue viably.\n",
    "\n",
    "Hence, the business issue that this extend looks for to fathom is the ought to distinguish the vital components and circumstances encompassing vehicle crashes in Chicago, in arrange to empower transportation specialists, law enforcement, policymakers, and protections companies to plan and actualize data-driven measures to decrease the event and affect of these crashes within the city. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS UNDERSTANDING \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key aspects of the business understanding for this project are as follows:\n",
    "\n",
    "**1.Significance of the Problem:**\n",
    "Vehicle crashes in Chicago are a major source of stress and financial burden for various stakeholders.\n",
    "These crashes result in loss of life, property damage, disruption of traffic flow, and significant economic losses.\n",
    "Effectively mitigating the frequency and severity of these crashes is a critical priority for the stakeholders involved.\n",
    "\n",
    "**2.Lack of Insights:**\n",
    "There is a lack of detailed insights into the root causes and impacts of vehicle crashes in Chicago.\n",
    "Without this knowledge, stakeholders are unable to develop and implement targeted strategies and interventions to address the problem effectively.\n",
    "\n",
    "**3.Stakeholder Needs:**\n",
    "Transportation experts, law enforcement, policymakers, and insurance companies require a comprehensive understanding of the key factors and circumstances surrounding vehicle crashes in Chicago.\n",
    "This understanding is necessary for these stakeholders to design and implement data-driven measures to reduce the occurrence and impact of these crashes.\n",
    "\n",
    "**4.Potential Benefits:**\n",
    "Improved road safety and reduced loss of life and property damage.Smoother traffic flow and reduced economic losses due to disruptions.Data-driven decision-making and targeted interventions by stakeholders to address the problem.Increased collaboration and coordination among stakeholders to address the issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUSINESS OBJECTIVE\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the projective objectives for the Vehicle Crash Analysis Project in Chicago, summarized into two key points:\n",
    "\n",
    "***1.Leverage Machine Learning and Simulation Capabilities:***\n",
    "Utilize advanced machine learning algorithms to develop predictive models that can accurately forecast the likelihood and severity of vehicle crashes based on various contributing factors, such as driver behavior, environmental conditions, and infrastructure characteristics.\n",
    "\n",
    "Integrate the predictive models with a simulation environment to create virtual scenarios of vehicle crashes in Chicago, and simulate the impacts of different interventions (e.g., changes in road infrastructure, traffic management policies, driver education programs) to assess their effectiveness in reducing the frequency and severity of crashes.\n",
    "\n",
    "***2.Develop a Collaborative Decision Support System for Stakeholders:***\n",
    "Design and implement a user-friendly decision support system that integrates the predictive models, simulation capabilities, and recommended interventions, providing stakeholders (transportation experts, law enforcement, policymakers, and insurance companies) with a comprehensive platform to explore the crash data, visualize the insights, and test the effectiveness of different strategies.\n",
    "\n",
    "Establish a collaborative framework that enables seamless communication and data sharing among the key stakeholders involved in road safety initiatives, and organize workshops, training sessions, and knowledge-sharing events to facilitate the adoption and integration of the project's insights and tools into the existing decision-making processes of the stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA UNDERSTANDING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of datasets from the Chicago Data Portal, an open data resource where one may find relevant information about the city. In particular we will be focusing on three datasets:Traffic Crashes - Vehicles, Traffic Crashes - People and Traffic Crashes - Crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1:Crash Data \n",
    "Description of Columns:\n",
    "Some of the columns found in the dataset are:\n",
    "\n",
    "* `CRASH_RECORD_ID`:This number can be used to link to the same crash in the Vehicles and People datasets. This number also serves as a unique ID in this dataset.\n",
    "\n",
    "\n",
    "* `CRASH_DATE`:Date and time of crash as entered by the reporting officer\n",
    "\n",
    "\n",
    "* `WEATHER_CONDITION`:Weather condition at time of crash, as determined by reporting officer\n",
    "\n",
    "\n",
    "* `ROADWAY_SURFACE_COND`:Road surface condition, as determined by reporting officer\n",
    "\n",
    "\n",
    "* `CRASH_TYPE`:A general severity classification for the crash. Can be either Injury and/or Tow Due to Crash or No Injury / Drive Away\n",
    "\n",
    "\n",
    "The full description of the columns can be found in the data_description.md file.\n",
    "\n",
    "Insights:When the weather is clear throughout the day, accidents tend to occur more frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2: Vehicles Data\n",
    "Description of Columns:\n",
    "Some of the columns found in the dataset are:\n",
    "\n",
    "* `CRASH_RECORD_ID`:This number can be used to link to the same crash in the Vehicles and People datasets. This number also serves as a unique ID in this dataset.\n",
    "\n",
    "\n",
    "* `CRASH_UNIT_ID`: A unique identifier for each vehicle record.\n",
    "\n",
    "\n",
    "* `MAKE`:The make (brand) of the vehicle, if relevant.\n",
    "\n",
    "\n",
    "* `VEHICLE_USE`: The normal use of the vehicle, if relevant.\n",
    "\n",
    "\n",
    "* `EXCEED_SPEED_LIMIT_I`:Indicator of whether the unit was speeding, as determined by the reporting officer.\n",
    "\n",
    "\n",
    "The full description of the columns can be found in the data_description.md file.\n",
    "\n",
    "Insights: Passanger vehicles used for personal use are more prone to accidents.We can also see that toyota and chevrolet makes are leading in number of accidents with 118 and 114 respectfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3: People Data\n",
    "Description of Columns:\n",
    "Some of the columns found in the dataset are:\n",
    "\n",
    "* `CRASH_RECORD_ID`:This number can be used to link to the same crash in the Vehicles and People datasets. This number also serves as a unique ID in this dataset.\n",
    "\n",
    "\n",
    "* `PERSON_ID`:A unique identifier for each person record. IDs starting with P indicate passengers. IDs starting with O indicate a person who was not a passenger in the vehicle (e.g., driver, pedestrian, cyclist, etc.).\n",
    "\n",
    "\n",
    "* `PERSON_TYPE`: Type of roadway user involved in crash\n",
    "\n",
    "\n",
    "* `SEX`: Gender of person involved in crash, as determined by reporting officer\n",
    "\n",
    "\n",
    "* `AGE`: Age of person involved in crash\n",
    "\n",
    "\n",
    "The full description of the columns can be found in the data_description.md file.\n",
    "\n",
    "Insights: Drivers are more prone to be affected with the accidents with 731 cases. Most victims are Males. The most affected age group is between 26-30.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this segment, we start by bringing in the fundamental Python libraries and modules. These apparatuses give basic capacities and capabilities that we are going utilize all through the investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data manipultion and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Data visualisation Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Machine learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Model Evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, roc_auc_score\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score, mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "# Handling Class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "#making map visuals\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have imported the desired Python libraries and modules, setting the establishment for our information examination. These instruments will empower us to perform a comprehensive investigation of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Reading The Datasets\n",
    "Here, we read and load the individual datasets into our analysis environment. This step is essential for preparing the data and making it accessible for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1349\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m                 h.request(req.get_method(), req.selector, req.data, headers,\n\u001b[0m\u001b[0;32m   1351\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_request\u001b[1;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[0;32m   1300\u001b[0m             \u001b[0mbody\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'body'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mendheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1011\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\http\\client.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             self.sock = self._context.wrap_socket(self.sock,\n\u001b[0m\u001b[0;32m   1425\u001b[0m                                                   server_hostname=server_hostname)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    499\u001b[0m         \u001b[1;31m# ctx._wrap_socket()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 500\u001b[1;33m         return self.sslsocket_class._create(\n\u001b[0m\u001b[0;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\ssl.py\u001b[0m in \u001b[0;36m_create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1039\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1310\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSSLCertVerificationError\u001b[0m: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9f6b2e229064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load data from CSV files into dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcrash\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://data.cityofchicago.org/resource/85ca-t3if.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mvehicle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://data.cityofchicago.org/resource/68nd-jvt3.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mperson\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://data.cityofchicago.org/resource/u6pd-qa9d.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;31m# though mypy handling of conditional imports is difficult.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    435\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;31m# TODO: fsspec can also handle HTTP via requests, but leaving this unchanged\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Content-Encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maudit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'urllib.Request'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;31m# post-process response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0m\u001b[0;32m    543\u001b[0m                                   '_open', req)\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttps_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mhttps_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1393\u001b[1;33m             return self.do_open(http.client.HTTPSConnection, req,\n\u001b[0m\u001b[0;32m   1394\u001b[0m                 context=self._context, check_hostname=self._check_hostname)\n\u001b[0;32m   1395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\learn-env\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mdo_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1351\u001b[0m                           encode_chunked=req.has_header('Transfer-encoding'))\n\u001b[0;32m   1352\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# timeout error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1353\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mURLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1354\u001b[0m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mURLError\u001b[0m: <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)>"
     ]
    }
   ],
   "source": [
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "# Load data from CSV files into dataframes\n",
    "crash = pd.read_csv('https://data.cityofchicago.org/resource/85ca-t3if.csv')\n",
    "vehicle = pd.read_csv('https://data.cityofchicago.org/resource/68nd-jvt3.csv')\n",
    "person = pd.read_csv('https://data.cityofchicago.org/resource/u6pd-qa9d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been effectively stacked into our investigation environment. This beginning step permits us to continue with information planning and investigation to pick up bits of knowledge into the combined dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Combining The datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we merge and combine the individual datasets into a single cohesive dataset. This consolidation simplifies the subsequent analysis and enables us to draw comprehensive conclusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f60e146de542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# merging all three databases into 1, observing shape and previewing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmerged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcrash\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvehicle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'crash_record_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"crash_record_id\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mperson\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'vehicle_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vehicle_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Display the shape and a preview of the merged dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# merging all three databases into 1, observing shape and previewing data \n",
    "merged = pd.merge(left=crash, right = vehicle, left_on='crash_record_id', right_on=\"crash_record_id\")\n",
    "df = pd.merge(left=merged, right=person, left_on = 'vehicle_id', right_on='vehicle_id')\n",
    "# Display the shape and a preview of the merged dataframe\n",
    "print(df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets have been successfully merged, resulting in a unified dataset that encompasses all relevant information. This consolidation facilitates a holistic analysis of the data and provides a clear foundation for subsequent steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Dropping Redundant columns\n",
    "\n",
    "We begin by identifying and addressing redundant columns that do not contribute meaningful information to the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-88e6bc0494e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m        \u001b[1;34m'model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vehicle_year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vehicle_use'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'travel_direction'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'maneuver'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'occupant_cnt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'first_contact_point'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m        'lic_plate_state', 'city']\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcleaned_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# dropping redundant columns, previewing shape, data and info \n",
    "drop = ['crash_record_id_x', 'crash_date_x', 'alignment', 'intersection_related_i', 'sec_contributory_cause',\n",
    "        'num_units','crash_unit_id', 'vehicle_id', 'person_id', \"crash_record_id_y\", 'street_no', 'street_direction',\n",
    "       'street_name', 'location', 'zipcode', 'crash_month', 'latitude', 'longitude', 'crash_date_y', 'unit_no',\n",
    "       'model', 'vehicle_year', 'vehicle_use', 'travel_direction', 'maneuver', 'occupant_cnt', 'first_contact_point',\n",
    "       'lic_plate_state', 'city']\n",
    "cleaned_df = df.drop(columns=drop)\n",
    "print(cleaned_df.shape)\n",
    "print(cleaned_df.head())\n",
    "cleaned_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-323fd3329fc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# calculating nulls percentage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnulls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Calculate the percentage of null values for columns with missing data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnull_percent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnulls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnulls\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# extracting columns with excesssive nulls which is set at 95%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ]
    }
   ],
   "source": [
    "# calculating nulls percentage\n",
    "nulls = cleaned_df.isna().sum()\n",
    "# Calculate the percentage of null values for columns with missing data\n",
    "null_percent = nulls[nulls>0] / len(df)\n",
    "# extracting columns with excesssive nulls which is set at 95%\n",
    "Index_label = null_percent[null_percent>.70].index.tolist()\n",
    "#displaying the first 7 columns with excessive nulls\n",
    "Index_label[:7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-9dc1f7a709f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dropping the columns with excessive nulls and previewing shape, data and info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcleaned_df2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIndex_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcleaned_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df' is not defined"
     ]
    }
   ],
   "source": [
    "# dropping the columns with excessive nulls and previewing shape, data and info \n",
    "cleaned_df2 = cleaned_df.drop(columns = Index_label)\n",
    "print(cleaned_df2.shape)\n",
    "print(cleaned_df2.head())\n",
    "cleaned_df2.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has undergone a thorough cleaning process, ensuring its integrity and reliability for analysis. We have eliminated redundant and irrelevant columns, handled missing values, and rectified any inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Irrelevant Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we recognize and drop unessential columns that have negligible affect on our examination. This prepare upgrades the dataset's significance and centers on the foremost powerful variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-91f37bd82449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         'crash_date', 'ejection','beat_of_occurrence']\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mcleaned_df3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df2' is not defined"
     ]
    }
   ],
   "source": [
    "# dropping following columns due to irrelevance in predicting the cause of car accidents \n",
    "# irrelevent columns were dropped due to column description\n",
    "columns = ['report_type', 'crash_type', 'bac_result', 'date_police_notified',\n",
    "        'injuries_incapacitating', 'most_severe_injury','injuries_non_incapacitating',\n",
    "        'injuries_reported_not_evident', 'injuries_no_indication', 'injuries_unknown', 'crash_date',\n",
    "        'crash_date', 'ejection','beat_of_occurrence']\n",
    "\n",
    "cleaned_df3 = cleaned_df2.drop(columns = columns)\n",
    "print(cleaned_df3.shape)\n",
    "cleaned_df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8c6f01125236>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Viewing columns after dropping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "#Viewing columns after dropping\n",
    "\n",
    "cleaned_df3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unessential columns have been effectively dropped from the dataset, streamlining the examination and emphasizing the key factors that contribute to our inquire about address. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling missing values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We address lost values within the dataset to guarantee precise and solid examination. This includes different procedures, such as ascription or evacuation, based on the nature of the lost information.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-4eaf426cd6c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Checking if there are columns with any other missing values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnull_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnull_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "#Checking if there are columns with any other missing values\n",
    "null_columns = cleaned_df3.columns[cleaned_df3.isnull().any()]\n",
    "null_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b18d7387235e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Filling Columns with the object datatype with the value missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobject_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'missing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "#Filling Columns with the object datatype with the value missing\n",
    "object_columns = list(cleaned_df3.select_dtypes(include=['object']).columns)\n",
    "for col in object_columns:\n",
    "    cleaned_df3[col] = cleaned_df3[col].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b18d7387235e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Filling Columns with the object datatype with the value missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mobject_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'missing'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "#Filling Columns with the object datatype with the value missing\n",
    "object_columns = list(cleaned_df3.select_dtypes(include=['object']).columns)\n",
    "for col in object_columns:\n",
    "    cleaned_df3[col] = cleaned_df3[col].fillna('missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling columns with numeric data with the median\n",
    "integer_columns = list(cleaned_df3.select_dtypes(include=['float','int64']).columns)\n",
    "for col in integer_columns:\n",
    "    median_age = df[col].median()\n",
    "    # Replace null values in the  column with the median\n",
    "    cleaned_df3[col] = cleaned_df3[col].fillna(median_age)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if there are columns with any other missing values\n",
    "null_columns = cleaned_df3.columns[cleaned_df3.isnull().any()]\n",
    "null_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the exploratory information examination stage, we dig into the dataset to reveal significant designs and experiences. We begin by conducting univariate investigation, looking at person variables' dispersions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the Univariate investigation stage of EDA, we center on analyzing person factors in segregation. This involves investigating the dispersion, central propensity, spread, and conceivable exceptions of each variable. By visualizing and summarizing one variable at a time, able to recognize key highlights and designs that contribute to the in general setting of the information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-85c28d8dc3cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Describing the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "#Describing the dataset\n",
    "cleaned_df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-61838501cdb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#Plotting a Histogram of Age against the Incident count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mkde\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Age against the Incident count\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#Plotting a Histogram of crush hour against the Incident count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(ncols=2,sharey=True, figsize=(11,4))\n",
    "#Plotting a Histogram of Age against the Incident count\n",
    "sns.histplot(cleaned_df3.age,kde=True,ax=ax1)\n",
    "ax1.set_title(\"Age against the Incident count\")\n",
    "#Plotting a Histogram of crush hour against the Incident count\n",
    "sns.histplot(cleaned_df3.crash_hour,kde=True,ax=ax2)\n",
    "ax2.set_title(\"Crush Hour against the Incident count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Univariate examination has given us with profitable bits of knowledge into the person variables' characteristics and disseminations. By visualizing histograms, thickness plots, and outline measurements, we have recognized imperative designs and propensities. For occasion, we watched that the 'posted_speed_limit' variable takes after a skewed dispersion, with most values concentrated around particular speed limits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building on the bits of knowledge picked up from Univariate examination, Bivariate examination dives more profound by investigating connections between sets of factors. By analyzing how factors connected with one another, we are able reveal relationships, affiliations, or potential causal connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e96408dccd30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Creating a scatter plot of age vs. injuries_total\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0max2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msharey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'injuries_total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Age against total injuries\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.show()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating a scatter plot of age vs. injuries_total\n",
    "fig, (ax1,ax2) = plt.subplots(ncols=2,sharey=True, figsize=(11,4))\n",
    "sns.scatterplot(x='age', y='injuries_total', data=cleaned_df3, ax=ax1)\n",
    "ax1.set_title(\"Age against total injuries\")\n",
    "#plt.show()\n",
    "# Create a bar plot of weather_condition vs. injuries_total\n",
    "sns.barplot(x='weather_condition', y='injuries_total', data=cleaned_df3, ax=ax2) \n",
    "plt.xticks(rotation=90)\n",
    "ax2.set_title(\"Weather conditions against total injuries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through Bivariate analysis, we have discovered meaningful relationships between pairs of variables. For instance, we observed a positive correlation between 'posted_speed_limit' and 'crash_severity,' indicating that higher posted speed limits are associated with more severe crashes. This finding provides a preliminary understanding of the factors influencing crash severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_df3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-af6af3facab1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# List of numeric columns for multivariate analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnumeric_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'float'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'int64'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Select a subset of significant numeric columns for analysis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msignificant_numeric_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'injuries_total'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'crash_hour'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'crash_day_of_week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_df3' is not defined"
     ]
    }
   ],
   "source": [
    "# List of numeric columns for multivariate analysis\n",
    "numeric_columns = list(cleaned_df3.select_dtypes(include=['float', 'int64']).columns)\n",
    "\n",
    "# Select a subset of significant numeric columns for analysis\n",
    "significant_numeric_columns = ['injuries_total', 'crash_hour', 'crash_day_of_week']\n",
    "\n",
    "# Create a pair plot\n",
    "sns.set(style=\"ticks\")\n",
    "pair_plot = sns.pairplot(cleaned_df3[significant_numeric_columns], diag_kind='kde')\n",
    "pair_plot.fig.suptitle(\"Pair Plot of Significant Numeric Variables\", y=1.02)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate examination has advertised a comprehensive see of how numerous factors connected and contribute to our inquire about address. By consolidating extra factors such as 'weather_condition,' 'roadway_surface_cond,' and 'crash_hour,' we were able to make a more nuanced understanding of the components affecting crash seriousness. This upgraded point of view helps in making educated choices and proposals.\n",
    "\n",
    "In outline, the Exploratory Information Investigation (EDA) stage has been instrumental in unraveling key experiences and building up a strong establishment for our examination. By efficiently looking at person factors, investigating connections between sets of factors, and amplifying our investigation to include numerous variables, we have picked up a more profound understanding of the dataset's subtleties. These bits of knowledge will direct our consequent modeling and decision-making forms, eventually driving to more precise and educated conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Modeling\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Modeling stage includes making prescient models based on the bits of knowledge picked up from the Exploratory Information Examination (EDA). In this stage, we utilize machine learning calculations to construct models that can make expectations or classifications based on the accessible information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Research Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the noteworthy variables affecting the seriousness of activity crashes, and can they be utilized to foresee the seriousness level (e.g., minor, direct, extreme)?\n",
    "\n",
    "We have carefully chosen significant highlights, such as 'posted_speed_limit,' 'weather_condition,' 'roadway_surface_cond,' and 'crash_hour,' which were recognized amid the EDA stage as possibly impactful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting a subset of columns for the model\n",
    "selected_columns = [\n",
    "    'posted_speed_limit', 'weather_condition', 'roadway_surface_cond', \n",
    "    'crash_hour', 'crash_day_of_week', 'unit_type', 'vehicle_type', \n",
    "    'state', 'age', 'safety_equipment', 'airbag_deployed', 'injury_classification'\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with selected columns\n",
    "w_df = cleaned_df3[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characteristics and Objective Defined: In this case, we extract the target (y) and features (X) from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = w_df.drop(columns=['injury_classification'])\n",
    "y = w_df['injury_classification']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-Test Split: A 75-25 split ratio was used to divide the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-923add241b4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train-test split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing: Now we Define preprocessing steps for numerical and categorical columns using StandardScaler and OneHotEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for numerical and categorical columns\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "num_transformer = StandardScaler()\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline Creation:We then Create a pipeline that combines preprocessing and the RandomForestClassifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline with preprocessing and RandomForestClassifier\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Models Fitting: The pipeline is applied to the training set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction and Evaluation: Using the confusion matrix and classification report, we forecast the results of the test set and assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst1 = X_test[X_test['posted_speed_limit'] > 30]\n",
    "tst1 = tst1.iloc[5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst1_pred = pipeline.predict(tst1)\n",
    "tst1_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst2 = X_test[X_test['weather_condition'] != 'CLEAR']\n",
    "tst2 = tst2.iloc[9:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst2_pred = pipeline.predict(tst2)\n",
    "tst2_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst4 = X_test[X_test['roadway_surface_cond'] != 'DRY']\n",
    "tst4 = tst4.iloc[5:10]\n",
    "tst4_pred = pipeline.predict(tst4)\n",
    "tst4_pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By preparing a RandomForestClassifier show and assessing its execution utilizing measurements like exactness, exactness, review, and F1-score, we have made a prescient show that can classify crash seriousness levels. The demonstrate accomplished an exactness of around 82%, showing its capacity to anticipate seriousness with a sensible level of accuracy. The classification report gives a nitty gritty breakdown of the model's execution over diverse seriousness classes, highlighting ranges where the show exceeds expectations and zones for potential advancement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viability of Security Hardware:\n",
    "\n",
    "\n",
    "Inquire about Address:\n",
    "What is the affect of security gear utilization (e.g., seatbelts, airbags) on the event and results of crashes, and can we measure their adequacy in lessening wounds?\n",
    "\n",
    "We have changed the dataset to form a twofold classification errand where 'injuries_class' speaks to whether wounds happened or not. This demonstrate points to evaluate the affect of security gear utilization on crash results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['safety_equipment','airbag_deployed','injury_classification','injuries_total']\n",
    "new_df = cleaned_df3[selected_columns].copy() \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature engineering \n",
    "#mapping specific values in the safety_equipment column to new values based on a defined mapping \n",
    "safety_equipment_map = {'USAGE UNKNOWN':'USAGE UNKNOWN','SAFETY BELT USED':'SAFETY BELT USED','NONE PRESENT':'NONE PRESENT','HELMET NOT USED':'HELMET NOT USED','missing':'USAGE UNKNOWN',\n",
    "                        'BICYCLE HELMET (PEDACYCLIST INVOLVED ONLY)': 'UNKNOWN/OTHER','CHILD RESTRAINT - FORWARD FACING': 'CHILD RESTRAINT','SAFETY BELT NOT USED': 'SAFETY BELT NOT USED','CHILD RESTRAINT - REAR FACING': 'CHILD RESTRAINT','DOT COMPLIANT MOTORCYCLE HELMET': 'UNKNOWN/OTHER','CHILD RESTRAINT - TYPE UNKNOWN': 'CHILD RESTRAINT',                       \n",
    "                        }\n",
    "new_df.safety_equipment = new_df.safety_equipment.map(safety_equipment_map)\n",
    "\n",
    "#mapping specific values in the airbag_deployed column to new values based on a defined mapping \n",
    "airbag_deployed_map = {'missing':'DEPLOYMENT UNKNOWN','DID NOT DEPLOY':'DID NOT DEPLOY','NOT APPLICABLE':'NOT APPLICABLE','DEPLOYMENT UNKNOWN':'DEPLOYMENT UNKNOWN',\n",
    "                        'DEPLOYED, COMBINATION':'DEPLOYED','DEPLOYED, FRONT': 'DEPLOYED','DEPLOYED, SIDE': 'DEPLOYED'                      \n",
    "                        }\n",
    "new_df.airbag_deployed = new_df.airbag_deployed.map(airbag_deployed_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating classes: 0 = not injured, 1 = injured\n",
    "new_df['injuries_total'] = new_df['injuries_total'].map(lambda x: 1 if x > 0 else 0)\n",
    "new_df.rename(columns={'injuries_total': 'injuries_class'}, inplace=True)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X, y for train-test-split\n",
    "X2 = new_df.drop(columns= 'injuries_class')\n",
    "y2 = new_df['injuries_class']\n",
    "\n",
    "# train-test-split, test_size = 25%, random_state = 42\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size = .25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = new_df.drop(columns='injuries_class').select_dtypes('number').columns.tolist()\n",
    "# creating a pipeline\n",
    "# RobustScaler will use the median to scale\n",
    "num_transform = Pipeline([('scale', RobustScaler())])\n",
    "\n",
    "cat_cols = new_df.drop(columns='injuries_class').select_dtypes('object').columns.tolist()\n",
    "# creating a pipeline\n",
    "# OneHotEncoder will scale the categorical data to a binary column\n",
    "cat_transform = Pipeline([('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine both pipelines into one using columntransformer \n",
    "preprocessing = ColumnTransformer([('num', num_transform, num_cols), \n",
    "                                  ('cat', cat_transform, cat_cols)])\n",
    "\n",
    "# preprocessing X_train and X_test\n",
    "X_train_tf = preprocessing.fit_transform(X_train2)\n",
    "X_test_tf = preprocessing.transform(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-bf121cedcce4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# accessing categorical columns from pipeline then converting to dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Get the pipeline named 'cat' from the preprocessing pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mslice_pipe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_transformers_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# Get the feature names after encoding for categorical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcat_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcat_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessing' is not defined"
     ]
    }
   ],
   "source": [
    "# accessing categorical columns from pipeline then converting to dataframe\n",
    "# Get the pipeline named 'cat' from the preprocessing pipeline\n",
    "slice_pipe = preprocessing.named_transformers_['cat']\n",
    "# Get the feature names after encoding for categorical features\n",
    "cat_features = slice_pipe.named_steps['encoder'].get_feature_names_out(cat_cols)\n",
    "# Create a DataFrame for the transformed training data\n",
    "X_train_tf = pd.DataFrame(X_train_tf,columns=[*num_cols, *cat_features])\n",
    "X_train_tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing y_train(classes) count \n",
    "y_train2.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-17b655cfc098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Using the SMOTE procedure to solve class imbalance problems\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_train3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Using the SMOTE procedure to solve class imbalance problems\n",
    "X_train_tf, y_train3 = SMOTE().fit_resample(X_train_tf, y_train2)\n",
    "y_train3.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-1c3d29c19d5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#baseline model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_tf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m params_grid = {'criterion': ['gini', 'entropy'], \n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_tf' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and fit a decision tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "#baseline model\n",
    "clf.fit(X_train_tf, y_train3)\n",
    "\n",
    "params_grid = {'criterion': ['gini', 'entropy'], \n",
    "              'max_depth': [2,4,6,8,10,12,20, None],\n",
    "              'min_samples_leaf':[1, 5, 10, 20, 50, 100]}\n",
    "\n",
    "# create grid search to find best combination\n",
    "grid = GridSearchCV(clf, params_grid, cv=3)\n",
    "\n",
    "# fit x_train and y_train to grid(tuned model) \n",
    "grid.fit(X_train_tf, y_train)\n",
    "\n",
    "# observe combination of best params \n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-6ea84744950a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Get predictions using a model with tuned model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_tf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate the confusion matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grid' is not defined"
     ]
    }
   ],
   "source": [
    "# Get predictions using a model with tuned model\n",
    "y_pred2 = grid.best_estimator_.predict(X_test_tf)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test2, y_pred2)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-78add684958b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate RMSE (Root Mean Squared Error) - applicable for regression tasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Calculate recall, precision, accuracy, and F1-score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate RMSE (Root Mean Squared Error) - applicable for regression tasks\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Calculate recall, precision, accuracy, and F1-score\n",
    "recall = recall_score(y_test2, y_pred2)\n",
    "precision = precision_score(y_test2, y_pred2)\n",
    "accuracy = accuracy_score(y_test2, y_pred2)\n",
    "f1 = f1_score(y_test2, y_pred2)\n",
    "\n",
    "print(\"RMSE:\", round(rmse,2))\n",
    "print(\"Recall:\", round(recall,2))\n",
    "print(\"Precision:\", round(precision,2))\n",
    "print(\"Accuracy:\", round(accuracy,2))\n",
    "print(\"F1-Score:\", round(f1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data where there were was safety gadgets and the airbag did deploy.\n",
    "test_data1 = X_test2[(X_test2['safety_equipment'] == 'SAFETY BELT USED') & (X_test2['airbag_deployed'] == 'DEPLOYED')]\n",
    "test_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting where there was safety gadgets and the airbag did deploy.\n",
    "tf_testdata_1 = preprocessing.transform(test_data1)\n",
    "y_pred4 = grid.best_estimator_.predict(tf_testdata_1)\n",
    "unique_values, counts = np.unique(y_pred4, return_counts=True)\n",
    "# Create a dictionary to store the results\n",
    "result_dict = dict(zip(unique_values, counts))\n",
    "\n",
    "# Print the count for each unique value\n",
    "for value, count in result_dict.items():\n",
    "    print(f\"The value {value} appears {count} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating test data where there were no safety gadgets and the airbag did not deploy.\n",
    "test_data2 = X_test2[(X_test2['safety_equipment'] != 'SAFETY BELT USED') & (X_test2['airbag_deployed'] != 'DEPLOYED')]\n",
    "test_data2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting where there was no or unknown safety gadgets and the airbag did not deploy.\n",
    "tf_testdata_2 = preprocessing.transform(test_data2)\n",
    "y_pred5 = grid.best_estimator_.predict(tf_testdata_2)\n",
    "unique_values, counts = np.unique(y_pred5, return_counts=True)\n",
    "# Create a dictionary to store the results\n",
    "result_dict = dict(zip(unique_values, counts))\n",
    "\n",
    "# Print the count for each unique value\n",
    "for value, count in result_dict.items():\n",
    "    print(f\"The value {value} appears {count} times.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the utilization of a DecisionTreeClassifier and a GridSearchCV for hyperparameter tuning, we have created a show that predicts the probability of wounds happening in a crash based on security hardware utilization and airbag deployment.We can see that the cases where security hardware and airbag did not deploy,there were more cases than where security contraptions were utilized and the airbag deployed(zero demonstrated cases where there was no case of harm). By assessing the model's execution utilizing measurements like review, exactness, exactness, and F1-score, we decided that the demonstrate shows direct prescient capability, with an precision of 78%. The F1-score gives a adjusted degree of accuracy and review, demonstrating the model's potential in distinguishing injury-prone scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic Hotspots and Avoidance Techniques:\n",
    "\n",
    "Investigate Address:\n",
    "Can we recognize geographic hotspots with higher crash frequencies, and what focused on avoidance procedures (e.g., moved forward signage, activity control) can be suggested\n",
    "\n",
    "This show leverages area information and activity control gadget data to get it crash designs and propose techniques to upgrade street security. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe\n",
    "loc_columns = ['location', 'longitude', 'latitude', 'traffic_control_device', 'injuries_total']\n",
    "loc_df = df[loc_columns].copy()\n",
    "print(loc_df.shape)\n",
    "loc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing the locations with the highest injury totals\n",
    "loc_df.groupby(['location'])['injuries_total'].sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_without_nulls = loc_df.dropna(how='any')\n",
    "rows_without_nulls = rows_without_nulls[rows_without_nulls['injuries_total']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "m = folium.Map(location=[41.8781,-87.6298], zoom_start=12)\n",
    "for item1, item2,item3 in zip(rows_without_nulls['latitude'], rows_without_nulls['longitude'],rows_without_nulls['injuries_total']):\n",
    "    folium.Marker([item1, item2], popup=f'Injuries:{item3}').add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating injury class: 0 = not injured, 1 = injured\n",
    "loc_df['injuries_total'] = loc_df['injuries_total'].map(lambda x: 1 if x > 0 else 0)\n",
    "loc_df.rename(columns={'injuries_total': 'injury'}, inplace=True)\n",
    "loc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X, y for train-test-split\n",
    "X_lr = loc_df.drop(columns= ['injury'])\n",
    "y_lr = loc_df['injury']\n",
    "\n",
    "# train-test-split, test_size = 25%, random_state = 42\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_lr, test_size = .2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder will scale the categorical data to a binary column\n",
    "cat_transform = Pipeline([('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing pipeline using columntransformer \n",
    "col1 = ['traffic_control_device']\n",
    "col2 = loc_df.drop(columns=['injury', 'traffic_control_device', 'location']).select_dtypes('object').columns.tolist()\n",
    "preprocessing = ColumnTransformer([('cat', cat_transform, col1)])\n",
    "\n",
    "# preprocessing X_train and X_test\n",
    "X_train_pp = preprocessing.fit_transform(X_train_lr)\n",
    "X_test_pp = preprocessing.transform(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accessing categorical columns from pipeline then converting to dataframe\n",
    "slice_pipe = preprocessing.named_transformers_['cat']\n",
    "cat_feat = slice_pipe.named_steps['encoder'].get_feature_names_out(col1)\n",
    "X_train_pp = pd.DataFrame(X_train_pp,columns=[*col2, *cat_feat])\n",
    "X_train_pp.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observing y_train(classes) count \n",
    "y_train_lr.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_pp, y_train_lr = smote.fit_resample(X_train_pp,y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit logisticregression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# fit the model\n",
    "log_reg.fit(X_train_pp, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and plot confussion matrix\n",
    "y_pred_lg = log_reg.predict(X_test_pp)\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "con_mat = confusion_matrix(y_test_lr, y_pred_lg)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "# Display the confusion matrix\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=con_mat, display_labels=['Class 0', 'Class 1'])\n",
    "display.plot(ax=ax1)\n",
    "ax1.set_title('Confusion Matrix')\n",
    "\n",
    "# ROC curve\n",
    "y_pred_prob = log_reg.predict_proba(X_test_pp)[::,1]\n",
    "fpr, tpr, _  = roc_curve(y_test_lr, y_pred_prob)\n",
    "auc = roc_auc_score(y_test_lr, y_pred_prob)\n",
    "\n",
    "#create ROC curve\n",
    "ax2.plot(fpr, tpr, label='AUC = {:.2f}'.format(auc))\n",
    "ax2.plot([0, 1], [0, 1], 'k--')\n",
    "ax2.set_xlabel('True positive rate')\n",
    "ax2.set_xlabel('False positive rate')\n",
    "ax2.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE (Root Mean Squared Error) - applicable for regression tasks\n",
    "rmse_lr = mean_squared_error(y_test_lr, y_pred_lg, squared=False)\n",
    "\n",
    "# Calculate recall, precision, accuracy, and F1-score\n",
    "recall_lr = recall_score(y_test_lr, y_pred_lg)\n",
    "precision_lr = precision_score(y_test_lr, y_pred_lg)\n",
    "accuracy_lr = accuracy_score(y_test_lr, y_pred_lg)\n",
    "f1_lr = f1_score(y_test_lr, y_pred_lg)\n",
    "\n",
    "print(\"RMSE:\", round(rmse_lr,2))\n",
    "print(\"Recall:\", round(recall_lr,2))\n",
    "print(\"Precision:\", round(precision_lr,2))\n",
    "print(\"Accuracy:\", round(accuracy_lr,2))\n",
    "print(\"F1-Score:\", round(f1_lr,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through calculated relapse modeling, we have made a prescient show that classifies whether a crash brought about in wounds or not based on the nearness of activity control gadgets at particular areas. The model's assessment measurements, counting review, accuracy, exactness, and F1-score, give experiences into its execution. With an exactness of 65%, the show illustrates potential in anticipating damage results based on the nearness of activity control gadgets. The Disarray Framework visualization makes a difference us get it the model's genuine positive and wrong positive rates, supporting in decision-making for street security enhancements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.The prescient models illustrated components that decide mischance seriousness, especially for cases including crippling wounds. There's room for encourage enhancement, particularly in distinguishing cases with detailed but not apparent wounds.\n",
    "\n",
    "2.Geographic investigation uncovered zones with higher crash frequencies, recommending focused on intercessions like upgraded activity requirement and foundation enhancements. The demonstrate anticipated that regions without activity control gadgets recorded wounds as compared to ranges with activity control gadgets.\n",
    "\n",
    "3.Street security In Chicago Pd is an progressing concern that requires ceaseless checking and advancement of techniques based on the information and changing circumstances. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing upon the perceptions and occasions specified over, we have the opportunity to define the consequent set of proposals:\n",
    "\n",
    "\n",
    "***1.Customary Checking and Opportune Upkeep of Street Surface Conditions:***\n",
    "It is unequivocally prompted to set up a orderly and careful approach to reliably screen and proactively keep up the condition of street surfaces. Quickly tending to issues such as potholes, breaks, and uneven surfaces is of vital significance to guarantee the security and consolation of drivers and people on foot.\n",
    "\n",
    "***2.Cultivating Collaboration with the Car Industry for Improved Vehicle Steadiness:***\n",
    "To synergize headways in street security and car innovation, a collaborative association between the transportation segment and the car industry is profoundly suggested. By mutually creating cutting-edge innovations that improve vehicle solidness, such as versatile suspension frameworks and progressed driver help highlights, the collective endeavors can abdicate momentous enhancements in generally street security.\n",
    "\n",
    "***3.Creating Weather-Responsive and Versatile Street Framework:***\n",
    "In arrange to viably relieve the affect of changing climate conditions on street security and toughness, it is prescribed to set out on the improvement of street foundation that illustrates a tall degree of responsiveness to shifting climatic variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
